Tokenizers & Analyzers
==========================

1ï¸âƒ£ The Processing Pipeline (Mental Model)
    Every text field goes through this exact pipeline:
        Raw text
          â†“
        Character filters (optional)
          â†“
        Tokenizer (mandatory)
          â†“
        Token filters (0..n)
          â†“
        Indexed tokens
    At query time, the same pipeline (or a different one) runs again.

    ğŸ“Œ Golden Rule
    If index-time and query-time analysis donâ€™t â€œagreeâ€, search breaks in subtle ways.


2ï¸âƒ£ Character Filters
    These run before tokenization. They change the raw string.
    Common ones
        html_strip â†’ removes HTML
        mapping â†’ replaces characters
        Example
            Text:
                C++ Developer & DevOps
                Mapping filter:
                "+" â†’ "plus"
                "&" â†’ "and"
            Becomes:
                Cplusplus Developer and DevOps
        Then tokenization happens.
    Use cases:
        normalizing weird input
        product SKUs
        logs with symbols

3ï¸âƒ£ Tokenizers (The Heart of Everything)
    A tokenizer decides:
        What is a â€œwordâ€?
    Letâ€™s go through the ones that matter in real systems.
    ğŸ”¹ standard (Default, Smart)
        Splits on:
            whitespace
            punctuation
            language rules
        Text:
            Quick-brown_fox@example.com
        Tokens:
            quick
            brown_fox
            example.com
        Best for:
            natural language
            logs
            descriptions

    ğŸ”¹ whitespace
        Splits ONLY on spaces.
        Text:
            quick-brown fox
        Tokens:
            quick-brown
            fox
        Use when:
            you want to preserve punctuation
            product codes
            semi-structured text

    ğŸ”¹ keyword (Special, Dangerous if Misused)
        Whole field = ONE token.
        Text:
            New York City
        Token:
            new york city
        Use for:
            IDs
            tags
            email
            enums
        This is basically how keyword fields behave internally.

    ğŸ”¹ pattern
        Split using regex.
        Example:
            Split on non-alphanumeric:
            [^a-zA-Z0-9]
        Text:
            ERR-404:DB_DOWN
        Tokens:
            ERR
            404
            DB
            DOWN
        Very powerful for:
            logs
            trace IDs
            codes

    ğŸ”¹ edge_ngram (Autocomplete King)
        Breaks words into prefixes.
        Text:
            search
        Tokens:
            s, se, sea, sear, searc, search
        Use for:
            search-as-you-type
            suggestions
        âš ï¸ High index size, must be controlled.

    ğŸ”¹ ngram (Contains Search)
        Breaks into chunks.
        Text:
            log
        Tokens:
            lo, og
        Use for:
            contains search
            fuzzy-ish matching
        âš ï¸ Very heavy. Use carefully.

4ï¸âƒ£ Token Filters (Meaning Changers)
    These run after tokenization.
    ğŸ”¹ lowercase
        ERROR â†’ error
        Almost always use this.

    ğŸ”¹ stop
        Removes common words:
            the, is, at, on
        Good for:
            long text
        Bad for:
            exact phrase matching
            legal / compliance text

    ğŸ”¹ stemmer
        running â†’ run
        errors â†’ error
        Good for:
            search engines
        Bad for:
            product names  [E.g., running shoes being stemmed to run shoes]
            logs

    ğŸ”¹ synonym
        This is enterprise-level power.
        Example:
            db, database, datastore
        All become:
            database
        Now searching â€œdb errorâ€ matches â€œdatabase failureâ€.
        âš ï¸ Hard to maintain. Version your synonym lists. [lemmatisation file]

----------

ğŸ§  What is an Analyzer? (Clear, No Jargon)
    Simple definition
        An analyzer is a named pipeline that tells Elasticsearch
        how to turn raw text into searchable tokens.
        Itâ€™s not one thing â€” itâ€™s a combination of three things:
    ************************************************************
    * Analyzer = Character Filters + Tokenizer + Token Filters *
    ************************************************************
    Think of it like a recipe.

    Why analyzers exist at all
        Elasticsearch does NOT search raw text.
        It searches tokens.
        So when you index this:
            "Database connection timeout"
        Elasticsearch must first decide:
            What are the â€œwordsâ€?
            Should case matter?
            Should â€œtimeoutsâ€ match â€œtimeoutâ€?
            Should â€œDBâ€ match â€œdatabaseâ€?
        The analyzer answers all of that.
        The Full Flow (Mental Model)
            Index time
                Raw text
                  â†“
                Analyzer runs
                  â†“
                Tokens stored in inverted index
            Query time
                Search text
                  â†“
                Analyzer runs (same or overridden)
                  â†“
                Tokens matched against inverted index
        ğŸ“Œ Golden Rule (repeat this to yourself):
            Elasticsearch never matches raw text â€” it matches analyzed tokens.

    What an analyzer contains (formally)
        An analyzer has exactly one:
            Tokenizer (mandatory)
            And optionally:
                0 or more character filters
                0 or more token filters
        So structurally:
            "analyzer": {
              "my_analyzer": {
                "type": "custom",
                "char_filter": [ ... ],
                "tokenizer": "standard",
                "filter": [ ... ]
              }
            }

    Built-in analyzers (youâ€™ve already been using them without realizing)
        standard (default)
        Pipeline:
            (no char filters)
            â†’ standard tokenizer
            â†’ lowercase filter
            â†’ stop filter (depending on language)
        That means when you write:
            "type": "text", You are implicitly saying: â€œUse the standard analyzerâ€
            we can override it if we want to by specifying a different analyzer. [E.g., "analyzer": "log_analyzer"]

    So:
        "message": {
          "type": "text",
          "analyzer": "log_analyzer"
        }
    Means:
        This is text, and I want THIS recipe applied.


    Concrete Example (Before / After)
        Field
            "message": {
              "type": "text",
              "analyzer": "standard"
            }
        Text:
            "DB Error at Server-01"
        Standard analyzer produces:
            db
            error
            at
            server-01
        If you switch to a custom analyzer with synonyms:
            db â†’ database
        Now tokens become: [removed "at" with stop filter, added synonym for "db"]
            database
            error
            server-01
        So a search for "database error" suddenly works.
        Thatâ€™s the power of the analyzer.



